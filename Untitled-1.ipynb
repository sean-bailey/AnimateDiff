{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: diffusers[torch]==0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install safetensors pillow accelerate einops omegaconf diffusers[torch]==0.11.1 transformers xformers==0.0.20 triton==2.0.0 torchtext==0.15.2 torchdata==0.6.1 torchaudio==2.0.2 torchvision==0.15.2 torch==2.0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting safetensors\n",
      "  Using cached safetensors-0.4.0-cp310-cp310-macosx_11_0_arm64.whl (425 kB)\n",
      "Installing collected packages: safetensors\n",
      "Successfully installed safetensors-0.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been resized and saved to ./outputimages/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the directory containing the images and the output directory\n",
    "input_directory = './'\n",
    "output_directory = './outputimages/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Iterate through all the files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Open the image\n",
    "        with Image.open(file_path) as img:\n",
    "            # Resize the image using the high-quality Lanczos resampling filter\n",
    "            img = img.resize((512, 512), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Construct the full output path\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            \n",
    "            # Save the image to the output directory\n",
    "            img.save(output_path)\n",
    "\n",
    "print(\"All images have been resized and saved to\", output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seanbailey/Github/AnimateDiff/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers.modeling_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/seanbailey/Github/AnimateDiff/Untitled-1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanbailey/Github/AnimateDiff/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m \u001b[39mimport\u001b[39;00m DDIMScheduler, EulerDiscreteScheduler, PNDMScheduler\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanbailey/Github/AnimateDiff/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m CLIPTextModel, CLIPTokenizer\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/seanbailey/Github/AnimateDiff/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39manimatediff\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munet\u001b[39;00m \u001b[39mimport\u001b[39;00m UNet3DConditionModel\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanbailey/Github/AnimateDiff/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39manimatediff\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline_animation\u001b[39;00m \u001b[39mimport\u001b[39;00m AnimationPipeline\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/seanbailey/Github/AnimateDiff/Untitled-1.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39manimatediff\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m save_videos_grid\n",
      "File \u001b[0;32m~/Github/AnimateDiff/animatediff/models/unet.py:15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfiguration_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ConfigMixin, register_to_config\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m ModelMixin\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseOutput, logging\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiffusers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m TimestepEmbedding, Timesteps\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'diffusers.modeling_utils'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime\n",
    "from safetensors import safe_open\n",
    "\n",
    "# Import your modules (make sure all dependencies are installed and properly imported)\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import DDIMScheduler, EulerDiscreteScheduler, PNDMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "from animatediff.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
    "from animatediff.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
    "\n",
    "# Initialize AnimateController\n",
    "controller = AnimateController()\n",
    "\n",
    "# Set input parameters manually\n",
    "stable_diffusion_path = \"./models/StableDiffusion/\"\n",
    "motion_module_path = \"./models/Motion_Module/mm_sd_v15_v2.ckpt\"\n",
    "base_model_path = \"./models/DreamBooth_LoRA/realisticVisionV51_v20Novae.safetensors\"\n",
    "lora_model_path = None  # or None\n",
    "lora_alpha = 0.8\n",
    "prompt = \"Peaceful, Serene, sunrise, gentle breeze through trees, beautiful, photorealistic, misty mountains\"\n",
    "negative_prompt = \"bad quality, nightmare, low quality,\"\n",
    "sampler = \"DDIM\"  # or other available samplers\n",
    "sample_steps = 25\n",
    "width = 512\n",
    "height = 512\n",
    "video_length = 16\n",
    "cfg_scale = 7.5\n",
    "seed = random.randint(1, 1e8)  # or a fixed seed\n",
    "\n",
    "# Update controller with selected models\n",
    "controller.update_stable_diffusion(stable_diffusion_path)\n",
    "controller.update_motion_module(motion_module_path)\n",
    "controller.update_base_model(base_model_path)\n",
    "if lora_model_path:\n",
    "    controller.update_lora_model(lora_model_path)\n",
    "\n",
    "# Set the path to your seed image\n",
    "seed_image_path = \"./outputimages/testlogoimage.png\"\n",
    "\n",
    "# Run the animation with the seed image\n",
    "result_video_path = controller.animate(\n",
    "    stable_diffusion_path,\n",
    "    motion_module_path,\n",
    "    base_model_path,\n",
    "    lora_alpha,\n",
    "    prompt, \n",
    "    negative_prompt, \n",
    "    sampler, \n",
    "    sample_steps, \n",
    "    width, \n",
    "    video_length, \n",
    "    height, \n",
    "    cfg_scale, \n",
    "    seed,\n",
    "    start_frame_path=seed_image_path  # Provide the seed image path here\n",
    ")\n",
    "\n",
    "\n",
    "# Display the result (assuming result_video_path is a valid path to the generated video)\n",
    "from IPython.display import Video\n",
    "Video(result_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
